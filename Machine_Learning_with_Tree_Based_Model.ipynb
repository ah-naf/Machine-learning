{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree Classification**\n",
        "\n",
        "- Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.\n",
        "- In a Decision tree, there are two nodes, which are the Decision Node and Leaf Node. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches.\n",
        "- The decisions or the test are performed on the basis of features of the given dataset.\n",
        "- It is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions.\n",
        "- It is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.\n",
        "- In order to build a tree, we use the CART algorithm, which stands for Classification and Regression Tree algorithm.\n",
        "- A decision tree simply asks a question, and based on the answer (Yes/No), it further split the tree into subtrees."
      ],
      "metadata": {
        "id": "pGwsQFbkP4hH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdGb9PEfOjnH"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y,\n",
        "                                                   test_size=0.2,\n",
        "                                                   stratify=y,\n",
        "                                                   random_state=1)\n",
        "\n",
        "dt = DecisionTreeClassifier(max_depth=2, random_state=1, criterion='gini')\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building Blocks of a Decision-Tree**\n",
        "\n",
        "- **Decision-Tree:** data structure consisting of a hierarchy of nodes\n",
        "- **Node:** question or prediction\n",
        "- **Root:** no parent node, question giving rise to two children nodes.\n",
        "- **Internal Node:** one parent node, question giving rise to two children nodes.\n",
        "- **Leaf:** one parent node, no children nodes. It gives the prediction"
      ],
      "metadata": {
        "id": "Vrn-r9HyStiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import DecisionTreeRegressor from sklearn.tree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeRegressor(max_depth=8,\n",
        "             min_samples_leaf=0.13,\n",
        "            random_state=3)\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Import mean_squared_error from sklearn.metrics as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Compute y_pred\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "# Compute mse_dt\n",
        "mse_dt = MSE(y_test, y_pred)\n",
        "\n",
        "# Compute rmse_dt\n",
        "rmse_dt = mse_dt ** (1/2)\n",
        "\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
      ],
      "metadata": {
        "id": "vUrOfQAjYxee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diagnose Bias and Variance**\n",
        "\n",
        "CV_Error = (E1 + ... + E10) / 10\n",
        "\n",
        "## **Diagnose Variance Problems**\n",
        "- If f^ suffers from **high variance**: CV error of f^ > training set error of f^\n",
        "- f^ is said to overfit the training set. To remedy overfitting:\n",
        "  - decrease model complexity,\n",
        "  - decrease max depth, increase min samples per leaf, ...\n",
        "  - gather more data\n",
        "\n",
        "## **Diagnose Bias Problems**\n",
        "- If f^ suffers from **high bias**: CV error of f^ is roughly equal to the training set of error of f^ which is >> desired error.\n",
        "- f^ is said to underfit the training set. To remedy undefitting:\n",
        "  - increase model complexity\n",
        "  - increase max depth, decrease min samples per leaf\n",
        "  - gather more relevant features"
      ],
      "metadata": {
        "id": "BpCNJXJDCMHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Set seed for reproducibility\n",
        "SEED = 123\n",
        "# Split data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=SEED)\n",
        "# Instantiate decision tree regressor and assign it to 'dt'\n",
        "dt = DecisionTreeRegressor(max_depth=4,\n",
        "                           min_samples_leaf=0.14,\n",
        "                           random_state=SEED)\n",
        "\n",
        "\n",
        "# Evaluate the list of MSE ontained by 10-fold CV\n",
        "# Set n_jobs to -1 in order to exploit all CPU cores in computation\n",
        "MSE_CV = - cross_val_score(dt, X_train, y_train, cv= 10,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           n_jobs = -1)\n",
        "# Fit 'dt' to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "# Predict the labels of training set\n",
        "y_predict_train = dt.predict(X_train)\n",
        "# Predict the labels of test set\n",
        "y_predict_test = dt.predict(X_test)\n",
        "\n",
        "'''\n",
        " if MSE_CV.mean() > MSE(y_train, y_predict_train) then it overfits\n",
        "'''"
      ],
      "metadata": {
        "id": "ciyLZl1lFMOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Learning**\n",
        "\n",
        "Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. Basic idea is to learn a set of classifiers (experts) and to allow them to vote.\n",
        "\n",
        "- Train different models on the same dataset.\n",
        "- Let each model make its predictions.\n",
        "- **Meta-model**: aggregates predctions of individual models\n",
        "- Final Prediction: more robust and less prone to errors\n",
        "- Best results: models are skillful in different ways"
      ],
      "metadata": {
        "id": "3FaOyNYVJBLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "SEED = -1\n",
        "\n",
        "# Split data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=SEED)\n",
        "\n",
        "lr = LogisticRegression(random_state=SEED)\n",
        "knn = KNN()\n",
        "dt = DecisionTreeClassifier(random_state=SEED)\n",
        "\n",
        "classifiers = [('Logistic Regression', lr),\n",
        "               ('K Nearest Neighbours', knn),\n",
        "               ('Classification Tree', dt)]\n",
        "\n",
        "for clf_name, clf in classifiers:\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred= clf.predict(X_test)\n",
        "\n",
        "  print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
        "\n"
      ],
      "metadata": {
        "id": "XFsOmclHJYKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a VotingClassifier 'vc'\n",
        "vc = VotingClassifier(estimators=classifiers)\n",
        "\n",
        "# Fit 'vc' to the traing set and predict test set labels\n",
        "vc.fit(X_train, y_train)\n",
        "y_pred = vc.predict(X_test)\n",
        "\n",
        "# Evaluate the test-set accuracy of 'vc'\n",
        "print('Voting Classifier: {.3f}'.format(accuracy_score(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "9xjFyv3KKTHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bagging**\n",
        "\n",
        "Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model. Bagging avoids overfitting of data and is used for both regression and classification models, specifically for decision tree algorithms."
      ],
      "metadata": {
        "id": "-nLvncQKvBBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=SEED)\n",
        "\n",
        "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=1)\n",
        "\n",
        "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
        "bc.fit(X_train, y_train)\n",
        "y_pred = bc.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "AizQ-pebvRVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**\n",
        "\n",
        "Random Forest is like bagging. In bagging, the base estimator can be any other model. But in Random Forest the base estimator is Decision Tree\n",
        "\n",
        "**Classification:**\n",
        "- Aggregated Prediction by majority voting\n",
        "- RandomForestClassifier in scikit-learn\n",
        "\n",
        "**Regression:**\n",
        "- Aggregates prediction through averaging\n",
        "- RandomForestRegressor in scikit-learn"
      ],
      "metadata": {
        "id": "Ku77-AC51Em6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "# Split data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=SEED)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "rmse_test = MSE(y_test, y_pred) ** (1/2)"
      ],
      "metadata": {
        "id": "KT2h8f7W1tcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Importance**\n",
        "\n",
        "Tree-based methods: enable measuring the importance of each feature in prediction.\n",
        "\n",
        "in sklearn:\n",
        "- how much the tree nodes use a particular feature (weighted average) to reduce impurity\n",
        "- accessed using the attribute feature_importance_"
      ],
      "metadata": {
        "id": "tfy124OW2WBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "importances_rf = pd.Series(rf.feature_importances_, index=X.column)\n",
        "sorted_importances_rf = importances_rf.sort_values()\n",
        "\n",
        "sorted_importances_rf.plot(kind='barh', color='lightgreen'); plt.show()\n"
      ],
      "metadata": {
        "id": "wsCGK4Tu2sqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Boosting**\n",
        "\n",
        "Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added."
      ],
      "metadata": {
        "id": "2gxerOydsXuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdaBoosting**"
      ],
      "metadata": {
        "id": "UkZF02GPyhUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import models and utility functions\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 1\n",
        "# Split data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=SEED)\n",
        "\n",
        "# Instantiate a classification-tree 'dt'\n",
        "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
        "# Instantiate an AdaBoost classifier 'adab_clf'\n",
        "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
        "\n",
        "# Fit 'adb_clf' to the training set\n",
        "adb_clf.fit(X_train, y_train)\n",
        "# Predict the test set probabilities of positive class\n",
        "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Evaluate test-set roc_auc_score\n",
        "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n"
      ],
      "metadata": {
        "id": "UkHVibrAtPCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting**"
      ],
      "metadata": {
        "id": "bEpWByYlykdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import  GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 1\n",
        "# Split data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=SEED)\n",
        "\n",
        "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
        "\n",
        "gbt.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gbt.predict(X_test)\n",
        "\n",
        "rmse_test = MSE(y_test, y_pred) ** (1/2)"
      ],
      "metadata": {
        "id": "8Qf6vSgayp1a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}