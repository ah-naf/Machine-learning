{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlsKkQ46H94U"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y,\n",
        "                                                   test_size=0.2, random_state=123)\n",
        "\n",
        "xg_cl = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                          n_estimators=10, seed=123)\n",
        "xg_cl.fit(X_train, y_train)preds = xg_cl.predict(X_test)\n",
        "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
        "\n",
        "print(\"accuracy: %f\" % (accuracy))accuracy: 0.78333\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Boosting**\n",
        "- Its not a specific machine learning algorithm\n",
        "- Concept that can be applied to a set of machine learning models\n",
        "- Ensemble meta-algorithm used to convert many weak models into a strong models"
      ],
      "metadata": {
        "id": "attuoIRaGgTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-validation in XGBoost example**"
      ],
      "metadata": {
        "id": "RKQzYk0IG2_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "params = {\"objective\": 'binary:logistic', 'max_depth': 4}\n",
        "\n",
        "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nflod=4, num_boost_round=10,\n",
        "                    metrics='error', as_pandas=True)\n",
        "\n",
        "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
      ],
      "metadata": {
        "id": "VdGFCYXOG7G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use XGBoost**\n",
        "- You have a large number of training samples\n",
        "  - Greater than 1000 training samples and less than 100 features\n",
        "  - The number of features < number of training samples\n",
        "- You have a mixture of categorical and numerica features\n",
        "  - Or just numeric features\n",
        "\n",
        "**When not to use**\n",
        "- Image recognition\n",
        "- Computer vision\n",
        "- Natural Language Processing and understanding problems\n",
        "- When the number of training samples is significantly smaller than the number of features"
      ],
      "metadata": {
        "id": "x8QzLtDyIzcA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vpibyixKUCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common loss functions and XGBoost**\n",
        "- Loss function names in xgboost:\n",
        "  - reg:linear - use for regression problems\n",
        "  - reg:logistic - use for classification problems when you want just decision, not probability\n",
        "  - binary:logistic - use when you want probability rather than decision\n",
        "\n",
        "**Base learners and why we need them**\n",
        "- XGBoost involves creating meta-model that is composed of many individual models that combine to give a final prediction\n",
        "- Two kinds of base learners: tree and linear"
      ],
      "metadata": {
        "id": "H0Do7p1WLGMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree as base learners**"
      ],
      "metadata": {
        "id": "g1O1NLT-L1qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2,\n",
        "                                                   random_state=123)\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10,\n",
        "                          seed=123)\n",
        "\n",
        "xg_reg.fit(X_train, y_train)\n",
        "preds = xg_reg.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
        "print(\"RMSE: %f\" % (rmse))\n"
      ],
      "metadata": {
        "id": "P2ReU8CGL4Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear base learners**"
      ],
      "metadata": {
        "id": "mXDflg5hMLlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2,\n",
        "                                                   random_state=123)\n",
        "\n",
        "DM_train = xgb.DMatrix(data=X_train,label=y_train)\n",
        "DM_test =  xgb.DMatrix(data=X_test,label=y_test)\n",
        "\n",
        "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
        "\n",
        "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=10)\n",
        "preds = xg_reg.predict(DM_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
        "print(\"RMSE: %f\" % (rmse))\n"
      ],
      "metadata": {
        "id": "6K7hfbJ6MP7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing individual XGBoost trees**"
      ],
      "metadata": {
        "id": "LKLCwTFGQVNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the first tree\n",
        "xgb.plot_tree(xg_reg, num_trees=0)\n",
        "plt.show()\n",
        "\n",
        "# Plot the fifth tree\n",
        "xgb.plot_tree(xg_reg, num_trees=4)\n",
        "plt.show()\n",
        "\n",
        "# Plot the last tree sideways\n",
        "xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2tCJVQyvQZ-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing feature importances**"
      ],
      "metadata": {
        "id": "x2UMZzT0Qhws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the feature importances\n",
        "xgb.plot_importance(xg_reg)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MLgJlna9Qk4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tuning in XGBoost**"
      ],
      "metadata": {
        "id": "DoSi0oRPQBBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuned model example**"
      ],
      "metadata": {
        "id": "0oB6nX57QEBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "tuned_params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1, 'max_depth': 5}\n",
        "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix,\n",
        "                               params=tuned_params, nfold=4, num_boost_round=200, metrics=\"rmse\",\n",
        "                               as_pandas=True, seed=123)\n",
        "\n",
        "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
      ],
      "metadata": {
        "id": "iTqnHTqOQGQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automated boosting round selection using early_stopping**"
      ],
      "metadata": {
        "id": "WoPhTQP2Q3-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation with early stopping: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)"
      ],
      "metadata": {
        "id": "sAvu5yf8Q4wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tree Tunable Parameters in XGBoost**\n",
        "\n",
        "XGBoost is a popular gradient boosting library that is widely used for machine learning tasks. It provides a variety of tunable parameters to help optimize model performance. Here are some of the most commonly used tree-related parameters in XGBoost:\n",
        "\n",
        "1. **Learning Rate (eta):**\n",
        "   - Controls the contribution of each tree to the final prediction.\n",
        "   - Lower values make the model more robust but require more trees for the same performance.\n",
        "\n",
        "   ```python\n",
        "   learning_rate = 0.1\n",
        "   ```\n",
        "\n",
        "2. **Number of Trees (n_estimators):**\n",
        "   - The number of boosting rounds (trees) to be run.\n",
        "\n",
        "   ```python\n",
        "   n_estimators = 100\n",
        "   ```\n",
        "\n",
        "3. **Maximum Depth of a Tree (max_depth):**\n",
        "   - The maximum depth of each tree in the ensemble.\n",
        "   - Higher values can lead to overfitting.\n",
        "\n",
        "   ```python\n",
        "   max_depth = 3\n",
        "   ```\n",
        "\n",
        "4. **Minimum Child Weight (min_child_weight):**\n",
        "   - Minimum sum of instance weight (hessian) needed in a child.\n",
        "   - It can be used to control over-fitting; higher values make the algorithm more conservative.\n",
        "\n",
        "   ```python\n",
        "   min_child_weight = 1\n",
        "   ```\n",
        "\n",
        "5. **Gamma (min_split_loss):**\n",
        "   - Minimum loss reduction required to make a further partition on a leaf node.\n",
        "   - It adds a regularization term to the cost function.\n",
        "\n",
        "   ```python\n",
        "   gamma = 0\n",
        "   ```\n",
        "\n",
        "6. **Subsample:**\n",
        "   - The fraction of samples used for fitting the individual trees.\n",
        "   - It helps prevent overfitting.\n",
        "\n",
        "   ```python\n",
        "   subsample = 1.0\n",
        "   ```\n",
        "\n",
        "7. **Column Subsampling by Tree (colsample_bytree):**\n",
        "   - The fraction of features that are randomly sampled for building each tree.\n",
        "   - It helps prevent overfitting.\n",
        "\n",
        "   ```python\n",
        "   colsample_bytree = 1.0\n",
        "   ```\n",
        "\n",
        "8. **Column Subsampling by Split (colsample_bylevel):**\n",
        "   - Similar to `colsample_bytree`, but applies to each level.\n",
        "\n",
        "   ```python\n",
        "   colsample_bylevel = 1.0\n",
        "   ```\n",
        "\n",
        "9. **L1 Regularization Term on Weights (alpha):**\n",
        "   - L1 regularization term on weights (analogous to Lasso regression).\n",
        "\n",
        "   ```python\n",
        "   alpha = 0\n",
        "   ```\n",
        "\n",
        "10. **L2 Regularization Term on Weights (lambda):**\n",
        "    - L2 regularization term on weights (analogous to Ridge regression).\n",
        "\n",
        "    ```python\n",
        "    lambda = 1\n",
        "    ```\n",
        "\n",
        "These parameters provide a good starting point for tuning an XGBoost model. However, the optimal values depend on the specific dataset and problem, so it's common to perform a more thorough hyperparameter search using techniques like grid search or randomized search."
      ],
      "metadata": {
        "id": "-C5dGSnRRl9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Tunable Parameters in XGBoost**\n",
        "\n",
        "XGBoost also supports linear models in addition to tree-based models. Linear models in XGBoost are created by adding a linear component to the boosting process. Here are some of the commonly used linear tunable parameters in XGBoost:\n",
        "\n",
        "1. **Linear Learning Rate (eta):**\n",
        "   - Similar to the tree-based version, it controls the step size shrinkage used to prevent overfitting.\n",
        "\n",
        "   ```python\n",
        "   eta = 0.1\n",
        "   ```\n",
        "\n",
        "2. **L1 Regularization on Weights (alpha):**\n",
        "   - L1 regularization term on the linear weights (similar to Lasso regularization).\n",
        "   - It helps prevent overfitting by encouraging sparsity in the linear model.\n",
        "\n",
        "   ```python\n",
        "   alpha = 0\n",
        "   ```\n",
        "\n",
        "3. **L2 Regularization on Weights (lambda):**\n",
        "   - L2 regularization term on the linear weights (similar to Ridge regularization).\n",
        "   - It helps prevent overfitting by penalizing large coefficients.\n",
        "\n",
        "   ```python\n",
        "   lambda = 1\n",
        "   ```\n",
        "\n",
        "4. **Column Subsampling by Tree (colsample_bytree):**\n",
        "   - The fraction of features that are randomly sampled for building each tree.\n",
        "   - In linear models, it controls the subsampling of features.\n",
        "\n",
        "   ```python\n",
        "   colsample_bytree = 1.0\n",
        "   ```\n",
        "\n",
        "5. **Column Subsampling by Split (colsample_bylevel):**\n",
        "   - Similar to `colsample_bytree`, but applies to each level in the boosting process.\n",
        "\n",
        "   ```python\n",
        "   colsample_bylevel = 1.0\n",
        "   ```\n",
        "\n",
        "6. **Subsample:**\n",
        "   - The fraction of samples used for fitting the individual trees.\n",
        "   - In linear models, it controls the subsampling of data.\n",
        "\n",
        "   ```python\n",
        "   subsample = 1.0\n",
        "   ```\n",
        "\n",
        "These linear parameters are typically used when you choose the 'gblinear' booster type in XGBoost. You can set the booster type using the 'booster' parameter:\n",
        "\n",
        "```python\n",
        "booster = 'gblinear'\n",
        "```\n",
        "\n",
        "As always, the optimal values for these parameters depend on the specific dataset and problem, and tuning may be necessary to achieve the best performance."
      ],
      "metadata": {
        "id": "ha5phr5CSIL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuning eta**"
      ],
      "metadata": {
        "id": "kisa7lxbShuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree (boosting round)\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
        "\n",
        "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
        "eta_vals = [0.001, 0.01, 0.1]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the eta\n",
        "for curr_val in eta_vals:\n",
        "\n",
        "    params[\"eta\"] = curr_val\n",
        "\n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
        "                        num_boost_round=10, early_stopping_rounds=5,\n",
        "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
      ],
      "metadata": {
        "id": "7Q2mEAIURwvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid Search with XGBoost**"
      ],
      "metadata": {
        "id": "niWPb6NRTn27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the parameter grid: gbm_param_grid\n",
        "gbm_param_grid = {\n",
        "    'colsample_bytree': [0.3, 0.7],\n",
        "    'n_estimators': [50],\n",
        "    'max_depth': [2, 5]\n",
        "}\n",
        "\n",
        "# Instantiate the regressor: gbm\n",
        "gbm = xgb.XGBRegressor()\n",
        "\n",
        "# Perform grid search: grid_mse\n",
        "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
        "                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
        "grid_mse.fit(X, y)\n",
        "\n",
        "# Print the best parameters and lowest RMSE\n",
        "print(\"Best parameters found: \", grid_mse.best_params_)\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
      ],
      "metadata": {
        "id": "jHZXqeOgTqlh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}