{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Common Data Problems**"
      ],
      "metadata": {
        "id": "iixgR0AytA99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning data is an essential part of data analysis and processing for several reasons:\n",
        "\n",
        "1. **Accuracy and Quality of Insights**: Dirty or unclean data can lead to inaccurate analyses and misleading insights. For instance, duplicates, incorrect entries, or outliers can skew results and lead to incorrect conclusions.\n",
        "\n",
        "2. **Data Integrity**: Inconsistent data can harm the integrity of your dataset. For example, if a dataset includes both \"USA\" and \"United States\" as country names, it could be interpreted as two separate entities, affecting analyses that depend on country-level data.\n",
        "\n",
        "3. **Efficiency in Analysis**: Cleaning data helps streamline the analysis process. Working with unclean data can be time-consuming and inefficient, as it might require additional checks and balances during the analysis to account for data quality issues.\n",
        "\n",
        "4. **Compatibility and Integration**: When combining data from multiple sources, it's crucial to have consistent and clean data to ensure seamless integration. Mismatched formats, scales, or coding schemes can lead to integration issues.\n",
        "\n",
        "5. **Decision Making**: Data-driven decision-making relies on high-quality data. Decisions based on unclean data can be flawed, leading to poor outcomes for businesses or research findings.\n",
        "\n",
        "6. **Machine Learning and Modeling**: For machine learning models, the quality of input data directly affects the performance and accuracy of the model. Garbage in, garbage out: poor quality data can result in ineffective models.\n",
        "\n",
        "7. **Compliance and Ethical Considerations**: Certain industries have regulations governing data quality. Non-compliance due to poor data quality can have legal repercussions. Ethically, it's important to ensure data accuracy, especially when it impacts people's lives.\n",
        "\n",
        "If data is not cleaned:\n",
        "\n",
        "- **Poor Quality Results**: Analyses will yield unreliable or incorrect results, which can misinform decision-making processes.\n",
        "- **Resource Wastage**: Time and resources may be wasted on correcting errors that could have been addressed at the data cleaning stage.\n",
        "- **System Failures**: In systems reliant on data, unclean data can lead to failures or suboptimal performance.\n",
        "- **Loss of Credibility**: For businesses or researchers, poor data quality can lead to a loss of credibility and trustworthiness."
      ],
      "metadata": {
        "id": "q72qPITGuvNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Type Contraints**"
      ],
      "metadata": {
        "id": "UmPJDjZ5wY6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Data Type      | Description                                               |\n",
        "|----------------|-----------------------------------------------------------|\n",
        "| `object`       | Text or mixed numeric and non-numeric values.             |\n",
        "| `int64`        | Integer variables, 64-bit integers.                       |\n",
        "| `float64`      | Floating-point numbers, double precision.                 |\n",
        "| `bool`         | Boolean values (`True` or `False`).                       |\n",
        "| `datetime64`   | Date and time values, unified into a datetime format.     |\n",
        "| `timedelta[ns]`| Difference between two `datetime64` values.               |\n",
        "| `category`     | Categorical data with a limited, fixed number of values.  |\n",
        "| `complex`      | Complex numbers (less common in typical data analysis).   |\n",
        "\n"
      ],
      "metadata": {
        "id": "p40y8_3wvjs2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQx_XUBksnT2",
        "outputId": "58213ba4-a3f4-4826-fec2-593c178be4a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25760 entries, 0 to 25759\n",
            "Data columns (total 10 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   ride_id          25760 non-null  int64 \n",
            " 1   duration         25760 non-null  object\n",
            " 2   station_A_id     25760 non-null  int64 \n",
            " 3   station_A_name   25760 non-null  object\n",
            " 4   station_B_id     25760 non-null  int64 \n",
            " 5   station_B_name   25760 non-null  object\n",
            " 6   bike_id          25760 non-null  int64 \n",
            " 7   user_type        25760 non-null  int64 \n",
            " 8   user_birth_year  25760 non-null  int64 \n",
            " 9   user_gender      25760 non-null  object\n",
            "dtypes: int64(6), object(4)\n",
            "memory usage: 2.0+ MB\n",
            "None\n",
            "count    25760.000000\n",
            "mean         2.008385\n",
            "std          0.704541\n",
            "min          1.000000\n",
            "25%          2.000000\n",
            "50%          2.000000\n",
            "75%          3.000000\n",
            "max          3.000000\n",
            "Name: user_type, dtype: float64\n",
            "count     25760\n",
            "unique        3\n",
            "top           2\n",
            "freq      12972\n",
            "Name: user_type_cat, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "ride_sharing = pd.read_csv('ride_sharing_new.csv')\n",
        "\n",
        "ride_sharing.rename(columns={'Unnamed: 0': 'ride_id'}, inplace=True)\n",
        "\n",
        "# Print the information of ride_sharing\n",
        "print(ride_sharing.info())\n",
        "\n",
        "# Print summary statistics of user_type column\n",
        "print(ride_sharing['user_type'].describe())\n",
        "\n",
        "# Convert user_type from integer to category\n",
        "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
        "\n",
        "# Write an assert statement confirming the change\n",
        "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
        "\n",
        "# Print new summary statistics\n",
        "print(ride_sharing['user_type_cat'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summing strings and concatenating numbers**"
      ],
      "metadata": {
        "id": "azcRVUH6v5qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip duration of minutes\n",
        "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
        "\n",
        "# Convert duration to integer\n",
        "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
        "\n",
        "# Write an assert statement making sure of conversion\n",
        "assert ride_sharing['duration_time'].dtype == 'int'\n",
        "\n",
        "# Print formed columns and calculate average ride duration\n",
        "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
        "print(ride_sharing['duration_time'].mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY6iGLiRv1UC",
        "outputId": "a797d829-43aa-4549-f88c-db84b723f652"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         duration duration_trim  duration_time\n",
            "0      12 minutes           12              12\n",
            "1      24 minutes           24              24\n",
            "2       8 minutes            8               8\n",
            "3       4 minutes            4               4\n",
            "4      11 minutes           11              11\n",
            "...           ...           ...            ...\n",
            "25755  11 minutes           11              11\n",
            "25756  10 minutes           10              10\n",
            "25757  14 minutes           14              14\n",
            "25758  14 minutes           14              14\n",
            "25759  29 minutes           29              29\n",
            "\n",
            "[25760 rows x 3 columns]\n",
            "11.389052795031056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Range Constraints**"
      ],
      "metadata": {
        "id": "J6-T6LkFwd5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How to Deal with out of range data?**\n",
        "\n",
        "Dealing with out-of-range data in a dataset requires careful consideration, and the strategy can vary depending on the context and the nature of your data. Here are some approaches:\n",
        "\n",
        "1. **Dropping Data**:\n",
        "   - **When to Use**: If the out-of-range data is deemed erroneous or not relevant to the analysis.\n",
        "   - **Consideration**: This approach reduces the size of the dataset, which might not be ideal if the dataset is already small.\n",
        "\n",
        "2. **Setting Custom Minimums and Maximums**:\n",
        "   - **When to Use**: When you have known thresholds or limits beyond which data points are not feasible or relevant.\n",
        "   - **Consideration**: This method involves clipping the data at the specified minimums and maximums, which might distort the distribution of the data.\n",
        "\n",
        "3. **Treat as Missing and Impute**:\n",
        "   - **When to Use**: If the out-of-range values are suspected to be missing or erroneous but you don't want to lose the data point entirely.\n",
        "   - **Consideration**: Imputation methods (mean, median, mode, or predictive modeling) can be used, but this might introduce bias, especially if the number of out-of-range values is significant.\n",
        "\n",
        "4. **Setting Custom Value Depending on Business Assumptions**:\n",
        "   - **When to Use**: When domain knowledge or business rules can provide a reasonable assumption about what these out-of-range values should be.\n",
        "   - **Consideration**: This requires a deep understanding of the domain and the data. The risk is that incorrect assumptions can lead to misleading analysis.\n",
        "\n",
        "Each of these methods has its advantages and drawbacks. The choice depends on the specific requirements of your analysis, the nature of your data, and the potential impact of the out-of-range data on your results. It's also important to document the chosen method and the rationale behind it for transparency and reproducibility."
      ],
      "metadata": {
        "id": "-ZaDGJjkx6X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "ride_sharing['tire_sizes'] = np.random.choice([27, 28, 29], size=len(ride_sharing))\n",
        "ride_sharing['tire_sizes'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IojOK4MyF_C",
        "outputId": "2aee4492-4d27-48e5-8ec6-4f4e7b6165fd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    27\n",
              "1    28\n",
              "2    28\n",
              "3    29\n",
              "4    28\n",
              "Name: tire_sizes, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tire_sizes to integer\n",
        "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
        "\n",
        "# Set all values above 27 to 27\n",
        "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
        "\n",
        "# Reconvert tire_sizes back to categorical\n",
        "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
        "\n",
        "# Print tire size description\n",
        "print(ride_sharing['tire_sizes'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2C1BmL_zssx",
        "outputId": "25cff8ce-9c60-46f6-cc72-d7efc39e9f04"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count     25760\n",
            "unique        1\n",
            "top          27\n",
            "freq      25760\n",
            "Name: tire_sizes, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random dates with random year, month, and day\n",
        "# For simplicity, let's define a range of years (e.g., 2010 to 2023)\n",
        "years = np.random.randint(2010, 2024, size=len(ride_sharing))\n",
        "months = np.random.randint(1, 13, size=len(ride_sharing))\n",
        "days = np.random.randint(1, 29, size=len(ride_sharing))  # using 28 to avoid invalid dates\n",
        "\n",
        "# Combine year, month, and day to form date strings\n",
        "random_dates = [\"{}-{:02d}-{:02d}\".format(year, month, day) for year, month, day in zip(years, months, days)]\n",
        "\n",
        "# Add to the DataFrame\n",
        "ride_sharing['ride_date'] = random_dates\n",
        "\n",
        "\n",
        "# Convert ride_date to date\n",
        "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "# Save today's date\n",
        "today = dt.date.today()\n",
        "\n",
        "# Set all in the future to today's date\n",
        "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
        "\n",
        "# Print maximum of ride_dt column\n",
        "print(ride_sharing['ride_dt'].max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKTA7hbNz7O6",
        "outputId": "c309b087-47d0-4789-fdf7-1b0202a9129f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Uniqueness Constraints**"
      ],
      "metadata": {
        "id": "J76Fllei1f7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How to find duplicate rows?**"
      ],
      "metadata": {
        "id": "SRHGZQoL3_4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find duplicate rows in a DataFrame, you can use the `.duplicated()` method in Pandas. This method helps identify rows that have the same values in all or a selection of columns. Here's a breakdown of how to use `.duplicated()`:\n",
        "\n",
        "1. **`subset` Parameter**:\n",
        "   - **Usage**: Specifies the columns for considering duplication.\n",
        "   - **Example**: `subset=['column1', 'column2']` will check for duplicates only based on 'column1' and 'column2'.\n",
        "\n",
        "2. **`keep` Parameter**:\n",
        "   - **Options**:\n",
        "     - `'first'`: Marks duplicates as `True` except for the first occurrence.\n",
        "     - `'last'`: Marks duplicates as `True` except for the last occurrence.\n",
        "     - `False`: Marks all duplicates as `True`.\n",
        "   - **Example**: `keep='first'` will mark all duplicate rows as `True` except for the first occurrence of the duplicate row.\n",
        "\n"
      ],
      "metadata": {
        "id": "fZ0pZ1U24DDP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kACmfLdc5sg8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find duplicates\n",
        "duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
        "\n",
        "# Sort your duplicated rides\n",
        "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
        "\n",
        "# Print relevant columns\n",
        "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B-DdEWd5j1d",
        "outputId": "7645891f-1887-4695-c2cc-42c5d7aaae99"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [ride_id, duration, user_birth_year]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How to treat duplicate rows?**"
      ],
      "metadata": {
        "id": "gSTYLAsv4Uti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To treat duplicate values in a DataFrame, you can use the `.drop_duplicates()` method in Pandas. This method removes duplicate rows based on all or a specified subset of columns. Here's how to use `.drop_duplicates()`:\n",
        "\n",
        "1. **`subset` Parameter**:\n",
        "   - **Usage**: Specifies the columns for considering duplication.\n",
        "   - **Example**: `subset=['column1', 'column2']` will check for duplicates based on 'column1' and 'column2' only.\n",
        "\n",
        "2. **`keep` Parameter**:\n",
        "   - **Options**:\n",
        "     - `'first'`: Drops duplicates except for the first occurrence.\n",
        "     - `'last'`: Drops duplicates except for the last occurrence.\n",
        "     - `False`: Drops all duplicates.\n",
        "   - **Example**: `keep='first'` will drop all duplicate rows except for the first occurrence of each duplicate set.\n",
        "\n",
        "3. **`inplace` Parameter**:\n",
        "   - **Usage**: Determines whether to drop duplicates in place or to return a new DataFrame.\n",
        "   - **Options**:\n",
        "     - `True`: The duplicates are dropped in place, modifying the original DataFrame.\n",
        "     - `False` (default): Returns a new DataFrame with duplicates dropped, leaving the original DataFrame unchanged.\n",
        "   - **Example**: `inplace=True` will modify the original DataFrame to remove duplicate rows.\n",
        "\n"
      ],
      "metadata": {
        "id": "Yj1mcHuQ4gM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop complete duplicates from ride_sharing\n",
        "ride_dup = ride_sharing.drop_duplicates()\n",
        "\n",
        "# Create statistics dictionary for aggregation function\n",
        "statistics = {'user_birth_year': 'min', 'duration_time': 'mean'}\n",
        "\n",
        "# Group by ride_id and compute new statistics\n",
        "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
        "\n",
        "# Find duplicated values again\n",
        "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
        "duplicated_rides = ride_unique[duplicates == True]\n",
        "\n",
        "# Assert duplicates are processed\n",
        "assert duplicated_rides.shape[0] == 0"
      ],
      "metadata": {
        "id": "ha9pkkUa70ZZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text and Categorical Data Problems**"
      ],
      "metadata": {
        "id": "0Q7147L5LRkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Value Consistency**"
      ],
      "metadata": {
        "id": "JNyZrQWeOM14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value consistency, especially in terms of capitalization, is crucial in data preprocessing to ensure that categorical data is accurately analyzed. Inconsistent capitalization, like 'married', 'Married', 'UNMARRIED', 'unmarried', can lead to misclassification of categories. Here's how you can address this issue:\n",
        "\n",
        "### Standardizing Text Data\n",
        "\n",
        "You can standardize the capitalization of your text data using methods like `.lower()`, `.upper()`, or `.title()` in Pandas. For your specific case, you might want to convert all entries to lowercase (or uppercase) for consistency.\n",
        "\n",
        "#### Example\n",
        "\n",
        "Assuming you have a DataFrame `df` and a column `marital_status` with varying capitalization:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'marital_status': ['married', 'Married', 'UNMARRIED', 'unmarried']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Standardizing to lowercase\n",
        "df['marital_status'] = df['marital_status'].str.lower()\n",
        "\n",
        "print(df)\n",
        "```\n",
        "\n",
        "This will convert all the values in the `marital_status` column to lowercase, making them consistent.\n",
        "\n",
        "### Dealing with Specific Cases\n",
        "\n",
        "In cases where you have specific rules for capitalization (like titles), you can use `.title()`:\n",
        "\n",
        "```python\n",
        "df['marital_status'] = df['marital_status'].str.title()\n",
        "```\n",
        "\n",
        "This would convert 'married' to 'Married', 'UNMARRIED' to 'Unmarried', etc.\n",
        "\n",
        "### Regular Expressions\n",
        "\n",
        "For more complex cases, regular expressions can be used to match patterns and transform data accordingly.\n",
        "\n",
        "\n",
        "### Removing Trailing Spaces and Standardizing Capitalization\n",
        "\n",
        "Assuming you have a DataFrame `df` with a column `marital_status`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'marital_status': ['married ', ' Married', 'UNMARRIED ', 'unmarried']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Remove leading and trailing spaces and standardize to lowercase\n",
        "df['marital_status'] = df['marital_status'].str.strip().str.lower()\n",
        "\n",
        "print(df)\n",
        "```\n",
        "\n",
        "In this code:\n",
        "- `.strip()` removes any leading and trailing spaces from the strings.\n",
        "- `.lower()` converts all the characters to lowercase for consistency.\n",
        "\n",
        "\n",
        "### Note\n",
        "\n",
        "- Always ensure that the transformation aligns with the context of your data.\n",
        "- It's a good practice to review a sample of your data after transformation to ensure it was applied as expected.\n",
        "- Be cautious with global transformations, as they can sometimes lead to misinterpretations of the data (e.g., transforming 'ID' to 'id' might be undesirable in some contexts)."
      ],
      "metadata": {
        "id": "v6eFKUMlOSxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "airlines = pd.read_csv('airlines_final.csv')\n",
        "\n",
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "\n",
        "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
        "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
        "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
        "\n",
        "# Remove white spaces from `dest_size`\n",
        "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
        "\n",
        "# Verify changes have been effected\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYiwVwiuPw5X",
        "outputId": "51b3036d-27f3-4415-cfb9-5a7e508c73d0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
            " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
            " 'Australia/New Zealand' 'middle east']\n",
            "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
            " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n",
            "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
            " 'europe' 'central/south america' 'australia/new zealand']\n",
            "['Hub' 'Small' 'Medium' 'Large']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Collapsing Data into Categories**"
      ],
      "metadata": {
        "id": "XUq2vGtROwm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collapsing data into categories is a common operation when you want to simplify or group numerical data into a more manageable form. In your example, you're using Pandas' `cut()` function to create a new column `income_group` based on ranges defined for the `household_income` column. Here's a step-by-step explanation:\n",
        "\n",
        "### Using `pd.cut()` to Create Income Groups\n",
        "\n",
        "1. **Define Ranges**: You define the bins or ranges into which you want to categorize your data. In your case, these ranges are `[0, 200000, 500000, np.inf]`. This means you're dividing the data into three groups: incomes from 0 to 200,000, 200,000 to 500,000, and above 500,000.\n",
        "\n",
        "2. **Define Group Names**: The labels for these ranges are defined as `['0-200K', '200K-500K', '500K+']`.\n",
        "\n",
        "3. **Create Income Group Column**: Using `pd.cut()`, you categorize the `household_income` into these defined bins and create a new column `income_group` in the `demographics` DataFrame.\n",
        "\n",
        "Here's how your code looks:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming demographics is an existing DataFrame with a column 'household_income'\n",
        "# Sample data (for demonstration)\n",
        "demographics = pd.DataFrame({\n",
        "    'household_income': [150000, 250000, 550000, 100000, 300000]\n",
        "})\n",
        "\n",
        "# Define ranges and group names\n",
        "ranges = [0, 200000, 500000, np.inf]\n",
        "group_names = ['0-200K', '200K-500K', '500K+']\n",
        "\n",
        "# Create income group column\n",
        "demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(demographics[['income_group', 'household_income']])\n",
        "```\n",
        "\n",
        "This code will output a DataFrame where each row's `household_income` is categorized into the appropriate `income_group`.\n",
        "\n",
        "### Important Notes\n",
        "\n",
        "- Ensure that your income data does not contain null values as `pd.cut()` does not handle NaN values by default.\n",
        "- The choice of bins (ranges) and labels (group names) should be made based on the context and distribution of your data, as well as the requirements of your analysis.\n",
        "- This method is useful for creating categorical variables from continuous variables, which can be beneficial for certain types of analysis, visualization, and modeling."
      ],
      "metadata": {
        "id": "u1UvKNTkPEt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ranges for categories\n",
        "label_ranges = [0, 60, 180, np.inf]\n",
        "label_names = ['short', 'medium', 'long']\n",
        "\n",
        "# Create wait_type column\n",
        "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges,\n",
        "                               labels = label_names)\n",
        "\n",
        "# Create mappings and replace\n",
        "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday',\n",
        "            'Thursday': 'weekday', 'Friday': 'weekday',\n",
        "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
        "\n",
        "airlines['day_week'] = airlines['day'].replace(mappings)"
      ],
      "metadata": {
        "id": "1-oDogx9Oh84"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cleaning text data**"
      ],
      "metadata": {
        "id": "0TZ_1uhqRTBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"Dr.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
        "\n",
        "# Replace \"Mr.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
        "\n",
        "# Replace \"Miss\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
        "\n",
        "# Replace \"Ms.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
        "\n",
        "# Assert that full_name has no honorifics\n",
        "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "CEu-9EEbR77P",
        "outputId": "476e9a63-7c52-4957-c7db-7bb0c8e9f1a5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'full_name'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-349626045602>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Replace \"Dr.\" with empty string \"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mairlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mairlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dr.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Replace \"Mr.\" with empty string \"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mairlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mairlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mairlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mr.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'full_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store length of each row in survey_response column\n",
        "resp_length = airlines['survey_response'].str.len()\n",
        "\n",
        "# Find rows in airlines where resp_length > 40\n",
        "airlines_survey = airlines[resp_length > 40]\n",
        "\n",
        "# Assert minimum survey_response length is > 40\n",
        "assert airlines_survey['survey_response'].str.len().min() > 40\n",
        "\n",
        "# Print new survey_response column\n",
        "print(airlines_survey['survey_response'])"
      ],
      "metadata": {
        "id": "fqIphG3wSXfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Data Problem**"
      ],
      "metadata": {
        "id": "I-pNXQRpa5Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Uniformity**"
      ],
      "metadata": {
        "id": "110xiEmXcYEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uniformity in data science refers to the consistency and standardization of data formats, types, and values within a dataset. Achieving uniformity is crucial for reliable data analysis and processing. Here are key aspects of uniformity in data science:\n",
        "\n",
        "1. **Consistent Formats**: Data should be in a consistent format throughout the dataset. For instance, dates should follow a single format (like 'YYYY-MM-DD'), and strings should have consistent capitalization.\n",
        "\n",
        "2. **Standardized Units**: Measurements should be standardized to a common unit system. For example, distances should be consistently in either kilometers or miles, not a mix of both.\n",
        "\n",
        "3. **Data Types**: The data type of each column should be uniform. Numerical data shouldn't be mixed with text in a single column, and categorical data should be distinguished from continuous data.\n",
        "\n",
        "4. **Value Ranges**: For numerical data, it's important that the values fall within expected ranges. This includes avoiding unrealistic or out-of-range values.\n",
        "\n",
        "5. **Encoding Categorical Data**: Categorical data should be uniformly encoded, either using one-hot encoding, label encoding, or similar methods, depending on the analysis needs.\n",
        "\n",
        "6. **Handling Missing Values**: The approach to missing data (e.g., using NaNs, zeros, or imputation) should be consistent across the dataset.\n",
        "\n",
        "7. **Treatment of Duplicates and Outliers**: The dataset should be uniformly treated for duplicates and outliers, either by removing, correcting, or accounting for them in analysis.\n",
        "\n",
        "Uniformity is important because inconsistent or non-standard data can lead to incorrect analysis and misleading results. It is often achieved through data cleaning and preprocessing steps before any analysis or modeling is done."
      ],
      "metadata": {
        "id": "RLIKgRbFcoKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "banking = pd.read_csv('banking_dirty.csv')\n",
        "\n",
        "# Find values of acct_cur that are equal to 'euro'\n",
        "acct_eu = banking['acct_cur'] == 'euro'\n",
        "\n",
        "# Convert acct_amount where it is in euro to dollars\n",
        "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
        "\n",
        "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
        "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
        "\n",
        "# Assert that only dollar currency remains\n",
        "assert banking['acct_cur'].unique() == 'dollar'"
      ],
      "metadata": {
        "id": "DRF9lt3ictXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uniform Date**"
      ],
      "metadata": {
        "id": "Q_G06vA4dqYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the header of account_opened\n",
        "print(banking['account_opened'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL_Uh0tKdGLO",
        "outputId": "79c14f98-4f1d-431e-e0c8-96865cc74012"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    02-09-18\n",
            "1    28-02-19\n",
            "2    25-04-18\n",
            "3    07-11-17\n",
            "4    14-05-18\n",
            "Name: account_opened, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the header of account_opend\n",
        "print(banking['account_opened'].head())\n",
        "\n",
        "# Convert account_opened to datetime\n",
        "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
        "                                           # Infer datetime format\n",
        "                                           infer_datetime_format = True,\n",
        "                                           # Return missing value for error\n",
        "                                           errors = 'coerce')\n",
        "\n",
        "# Get year of account opened\n",
        "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
        "\n",
        "# Print acct_year\n",
        "print(banking['acct_year'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQT_yjAXdkwK",
        "outputId": "c934880d-dcca-4e39-eb58-4f12c36b99cc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    02-09-18\n",
            "1    28-02-19\n",
            "2    25-04-18\n",
            "3    07-11-17\n",
            "4    14-05-18\n",
            "Name: account_opened, dtype: object\n",
            "0     2018\n",
            "1     2019\n",
            "2     2018\n",
            "3     2017\n",
            "4     2018\n",
            "      ... \n",
            "95    2018\n",
            "96    2017\n",
            "97    2017\n",
            "98    2017\n",
            "99    2017\n",
            "Name: acct_year, Length: 100, dtype: object\n"
          ]
        }
      ]
    }
  ]
}