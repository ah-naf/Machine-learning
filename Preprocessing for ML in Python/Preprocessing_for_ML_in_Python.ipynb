{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploring Missing Data**"
      ],
      "metadata": {
        "id": "M0XS4UlntvaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbzK4wpVasyN",
        "outputId": "b9058bc3-7858-4eb6-cea8-7d3e193c5cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(617, 33)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "volunteer = pd.read_csv('volunteer_opportunities.csv')\n",
        "\n",
        "# Drop the Latitude and Longitude columns from volunteer\n",
        "volunteer_cols = volunteer.drop([\"Latitude\", \"Longitude\"], axis=1)\n",
        "\n",
        "# Drop rows with missing category_desc values from volunteer_cols\n",
        "volunteer_subset = volunteer_cols.dropna(subset=[\"category_desc\"])\n",
        "\n",
        "# Print out the shape of the subset\n",
        "print(volunteer_subset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Working With Data Types**"
      ],
      "metadata": {
        "id": "r_TXPlmTtzAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the head of the hits column\n",
        "print(volunteer[\"hits\"].head())\n",
        "\n",
        "# Convert the hits column to type int\n",
        "volunteer[\"hits\"] = volunteer['hits'].astype('int')\n",
        "\n",
        "# Look at the dtypes of the dataset\n",
        "print(volunteer.dtypes)"
      ],
      "metadata": {
        "id": "XnQaxuSxtq8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training and Test Sets**"
      ],
      "metadata": {
        "id": "aSoono2GuzQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why use stratify?**"
      ],
      "metadata": {
        "id": "1b1lOJ5Ru21x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `stratify` parameter in the `train_test_split` function of scikit-learn is used to ensure that the train and test sets have approximately the same percentage of samples of each target class as the complete set.\n",
        "\n",
        "\n",
        "In many real-world problems, especially in classification tasks, the distribution of classes might be imbalanced or skewed. For instance, in a dataset for disease diagnosis, the majority of samples might be of the 'no disease' class with a small fraction of 'disease' class. Without stratification, there's a risk that the train or test set might end up having an unrepresentative distribution of classes, which can bias the model training and evaluation."
      ],
      "metadata": {
        "id": "kEpXg7Cru6x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a DataFrame with all columns except category_desc\n",
        "X = volunteer_subset.drop(['category_desc'], axis=1)\n",
        "\n",
        "# Create a category_desc labels dataset\n",
        "y = volunteer_subset[['category_desc']]\n",
        "\n",
        "# Use stratified sampling to split up the dataset according to the y dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Print the category_desc counts from y_train\n",
        "print(y_train['category_desc'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D95q-2cVvsf1",
        "outputId": "498aed00-9ecf-4d3c-dc99-c78282ac0817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Strengthening Communities    230\n",
            "Helping Neighbors in Need     89\n",
            "Education                     69\n",
            "Health                        39\n",
            "Environment                   24\n",
            "Emergency Preparedness        11\n",
            "Name: category_desc, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Standarization**"
      ],
      "metadata": {
        "id": "BuqAboi0wOdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization, Log Normalization, and Feature Scaling are techniques used in data preprocessing, especially in machine learning, to make data more suitable for use in these models. Here's a brief overview of each:\n",
        "\n",
        "1. **Standardization**:\n",
        "   - **Description**: Standardization involves rescaling the features so that they have a mean of 0 and a standard deviation of 1. It's achieved by subtracting the mean and then dividing by the standard deviation.\n",
        "   - **When to Use**: It's particularly useful when the features in a dataset have different units or very different scales. Standardization is also important when using algorithms that are sensitive to the scale of the data, like Support Vector Machines (SVMs), k-Nearest Neighbors (k-NN), and Principal Component Analysis (PCA).\n",
        "\n",
        "\n",
        "In summary, standardization, log normalization, and feature scaling are essential techniques in the preprocessing phase of a machine learning pipeline. They help normalize data, reduce skewness, and ensure that the algorithm treats all features equally. The choice of technique largely depends on the nature of your data and the specific requirements of the machine learning algorithm you are using."
      ],
      "metadata": {
        "id": "HTt3gdMRwRl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variance and Bias**"
      ],
      "metadata": {
        "id": "nSOREKdbzIgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-low variance and bias in machine learning are concepts that describe the types of errors a model can have. Let's break them down into simpler terms:\n",
        "\n",
        "1. **Bias**:\n",
        "   - **High Bias**: This means the model is overly simplistic. It doesn't learn enough from the training data, sort of like not paying enough attention in class. Because of this, it makes a lot of mistakes both on the training data and on new, unseen data. This is known as underfitting.\n",
        "   - **Low Bias**: In this case, the model pays more attention to the training data and learns it quite well. It makes fewer mistakes on the training data, meaning it fits the training data better.\n",
        "\n",
        "2. **Variance**:\n",
        "   - **High Variance**: This happens when the model is like a student who only focuses on memorizing everything for the test and doesn’t really understand the concepts. It performs really well on the training data (like acing a test it saw before) but poorly on new, unseen data because it's too focused on the specific examples it was trained on. This is overfitting.\n",
        "   - **Low Variance**: A model with low variance doesn’t change its prediction method much with different training data. It's like a student with a solid understanding of concepts, who can answer different types of questions on the same topic.\n",
        "\n",
        "In an ideal world, you want a model with low bias and low variance, which means it learns well from the training data and generalizes well to new data. However, in reality, there's usually a trade-off between bias and variance (known as the bias-variance tradeoff). Balancing them is key to building a good predictive model."
      ],
      "metadata": {
        "id": "M1RcnK9HzOuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Log Normalization**"
      ],
      "metadata": {
        "id": "lyr0TDKqzxap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Description**: This technique involves applying the natural logarithm to your data to help manage skewed distributions. It can make highly skewed distributions less skewed and can help with making patterns more interpretable and accessible.\n",
        "   - **When to Use**: Log normalization is useful when you have data with a wide range of values, or when a few extreme values are dominating the learning process and you want to reduce their impact."
      ],
      "metadata": {
        "id": "KymHv_3M0Vr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "wine = pd.read_csv('wine_types.csv')\n",
        "\n",
        "# Print out the variance of the Proline column\n",
        "print(wine['Proline'].var())\n",
        "\n",
        "# Apply the log normalization function to the Proline column\n",
        "wine['Proline_log'] = np.log(wine['Proline'])\n",
        "\n",
        "# Check the variance of the normalized Proline column\n",
        "print(wine['Proline_log'].var())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N7GUi5iz1k1",
        "outputId": "b9e60871-e5f9-4ae8-94a3-b6201e59be28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99166.71735542436\n",
            "0.17231366191842012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Scaling**"
      ],
      "metadata": {
        "id": "3FSwJZ3j0cxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Description**: Feature scaling is a broader term that includes methods to scale the range of features. It typically involves scaling features to a range, for example, 0 to 1. This is known as Min-Max scaling.\n",
        "   - **When to Use**: Like standardization, feature scaling is critical when using algorithms that are sensitive to the magnitude of values and where distance between data points is important (e.g., k-NN, SVM, neural networks). It's also useful for speeding up gradient descent in learning algorithms."
      ],
      "metadata": {
        "id": "9OVo9P0D0e4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Subset the DataFrame you want to scale\n",
        "wine_subset = wine[[\"Ash\", \"Alcalinity of ash\", \"Magnesium\"]]\n",
        "\n",
        "# Apply the scaler to wine_subset\n",
        "wine_subset_scaled = scaler.fit_transform(wine_subset)"
      ],
      "metadata": {
        "id": "DAJJQTmq1gdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Standarized data and modeling**"
      ],
      "metadata": {
        "id": "m5oL2RnL2Hjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN on unscaled data**"
      ],
      "metadata": {
        "id": "3W980CuV22sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "scaler = StandardScaler()\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "print(knn.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "WIuLY98p2K43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN on scaled data**"
      ],
      "metadata": {
        "id": "T02CEl7K3C0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Instantiate a StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the training and test features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fit the k-nearest neighbors model to the training data\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Score the model on the test data\n",
        "print(knn.score(X_test_scaled, y_test))"
      ],
      "metadata": {
        "id": "3QZBkNSR3E3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering**"
      ],
      "metadata": {
        "id": "6h9ZPmHFO22r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **When to use LabelEncoder or OneHotEncoding?**"
      ],
      "metadata": {
        "id": "Q4zu4GK6PIGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LabelEncoder and One Hot Encoding are techniques used for encoding categorical variables in machine learning. Deciding which one to use depends on the specific requirements of your dataset and the model you are using. Here are some guidelines:\n",
        "\n",
        "1. **LabelEncoder**:\n",
        "   - **When to Use**: LabelEncoder is typically used when the categorical feature is ordinal, meaning there is a clear ordering in the categories. For example, sizes like Small, Medium, Large can be encoded as 0, 1, 2.\n",
        "   - **How it Works**: It simply converts each category into an integer. This is useful for models that can interpret the ordinal nature of the variable.\n",
        "   - **Limitations**: It can introduce a new problem in the model, as it implies an order among the categories which might not exist (e.g., encoding cities or country names).\n",
        "\n",
        "2. **One Hot Encoding**:\n",
        "   - **When to Use**: One Hot Encoding is used when the categorical feature is nominal (i.e., there is no intrinsic ordering of the categories). For instance, color categories such as Red, Blue, Green.\n",
        "   - **How it Works**: It creates a new binary column for each category in the original variable. Each observation gets a 1 in the column of its corresponding category and 0 in all other new columns.\n",
        "   - **Limitations**: This can lead to a dramatic increase in the dataset’s dimensionality (known as the curse of dimensionality), especially if the categorical variable has many unique categories.\n",
        "\n",
        "3. **Considerations for Model Type**:\n",
        "   - Some models like tree-based algorithms (e.g., Decision Trees, Random Forests) can work well with ordinal encodings, as they can handle the intrinsic ordering.\n",
        "   - Other models, particularly linear models (like Logistic Regression), and neural networks usually work better with One Hot Encoding as they treat each feature independently.\n",
        "\n",
        "4. **Dataset Size and Computational Constraints**:\n",
        "   - One Hot Encoding can be computationally expensive for variables with a large number of categories. In such cases, techniques like embedding or dimensionality reduction might be more efficient.\n",
        "\n",
        "5. **Hybrid Approaches**:\n",
        "   - Sometimes, a hybrid approach might be useful, like using Label Encoding for features with high cardinality and One Hot Encoding for features with fewer categories.\n",
        "\n",
        "In summary, the choice between LabelEncoder and One Hot Encoding depends on the nature of the categorical variable (ordinal vs. nominal), the type of model you are using, and the size and computational constraints of your dataset."
      ],
      "metadata": {
        "id": "cZrkSkGuPOza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding categorical variables - binary**"
      ],
      "metadata": {
        "id": "EYSZqe14PS1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "hiking = pd.read_json('hiking.json')\n",
        "\n",
        "# Set up the LabelEncoder object\n",
        "enc = LabelEncoder()\n",
        "\n",
        "# Apply the encoding to the \"Accessible\" column\n",
        "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
        "\n",
        "# Compare the two columns\n",
        "print(hiking[['Accessible', 'Accessible_enc']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKfNhiIOPWWQ",
        "outputId": "cb623ecb-2346-4861-8349-ab01fa862894"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Accessible  Accessible_enc\n",
            "0          Y               1\n",
            "1          N               0\n",
            "2          N               0\n",
            "3          N               0\n",
            "4          N               0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding categorical variables - one hot encoding**"
      ],
      "metadata": {
        "id": "VVOX1ynDPzBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the category_desc column\n",
        "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
        "\n",
        "# Take a look at the encoded columns\n",
        "print(category_enc.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4NS7GSZP2iV",
        "outputId": "b6f7be98-79e3-4865-d036-cbe8c636d49e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Education  Emergency Preparedness  Environment  Health  \\\n",
            "0          0                       0            0       0   \n",
            "1          0                       0            0       0   \n",
            "2          0                       0            0       0   \n",
            "3          0                       0            0       0   \n",
            "4          0                       0            1       0   \n",
            "\n",
            "   Helping Neighbors in Need  Strengthening Communities  \n",
            "0                          0                          0  \n",
            "1                          0                          1  \n",
            "2                          0                          1  \n",
            "3                          0                          1  \n",
            "4                          0                          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Putting it all together**"
      ],
      "metadata": {
        "id": "SIXIRuR1m6Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking column types**"
      ],
      "metadata": {
        "id": "oR802MO0m90H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "ufo = pd.read_csv('ufo_sightings_large.csv')\n",
        "\n",
        "# Print the DataFrame info\n",
        "print(ufo.info())\n",
        "\n",
        "# Change the type of seconds to float\n",
        "ufo[\"seconds\"] = ufo['seconds'].astype('float')\n",
        "\n",
        "# Change the date column to type datetime\n",
        "ufo[\"date\"] = pd.to_datetime(ufo['date'])\n",
        "\n",
        "# Check the column types\n",
        "print(ufo.info())"
      ],
      "metadata": {
        "id": "n1n_a4RSm__3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropping Missing Data**"
      ],
      "metadata": {
        "id": "ybG5dog_nTcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
        "print(ufo[['length_of_time', 'state', 'type']].isna().sum())\n",
        "\n",
        "# Drop rows where length_of_time, state, or type are missing\n",
        "ufo_no_missing = ufo.dropna(subset=['length_of_time', 'state', 'type'])\n",
        "\n",
        "# Print out the shape of the new dataset\n",
        "print(ufo_no_missing.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YohSVQclnVRZ",
        "outputId": "b9840c19-b762-4749-dc4f-b7fc79ad40f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length_of_time    143\n",
            "state             419\n",
            "type              159\n",
            "dtype: int64\n",
            "(4283, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting numbers from strings**"
      ],
      "metadata": {
        "id": "-zCLqecZouK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ufo['length_of_time']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuTxGY3lo6xh",
        "outputId": "d426c16c-14f5-40c4-8057-76a7506b4a3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0               2 weeks\n",
              "1                30sec.\n",
              "2                   NaN\n",
              "3       about 5 minutes\n",
              "4                     2\n",
              "             ...       \n",
              "4930    about 5 seconds\n",
              "4931         25 seconds\n",
              "4932      early morning\n",
              "4933            2 hours\n",
              "4934          1 minutes\n",
              "Name: length_of_time, Length: 4935, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def return_minutes(time_string):\n",
        "\n",
        "    # Search for numbers in time_string\n",
        "    num = re.search(r'\\d+', time_string)\n",
        "    if num is not None:\n",
        "        return int(num.group(0))\n",
        "\n",
        "# Apply the extraction to the length_of_time column\n",
        "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
        "\n",
        "# Take a look at the head of both of the columns\n",
        "print(ufo[['length_of_time', 'minutes']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "V17xd3msowyL",
        "outputId": "5255d1da-b57f-46eb-a1f7-b9bd10bb7793"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 1), match='2'>\n",
            "<re.Match object; span=(0, 2), match='30'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b05e889f523e>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Apply the extraction to the length_of_time column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mufo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"minutes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mufo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"length_of_time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_minutes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Take a look at the head of both of the columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b05e889f523e>\u001b[0m in \u001b[0;36mreturn_minutes\u001b[0;34m(time_string)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Search for numbers in time_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    199\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply log normalization**"
      ],
      "metadata": {
        "id": "K6h0iqBRqFRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the variance of the seconds and minutes columns\n",
        "print(ufo[['seconds', 'minutes']].var())\n",
        "\n",
        "# Log normalize the seconds column\n",
        "ufo[\"seconds_log\"] = np.log(ufo['seconds'])\n",
        "\n",
        "# Print out the variance of just the seconds_log column\n",
        "print(ufo['seconds_log'].var())"
      ],
      "metadata": {
        "id": "qM3YUElSqGJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding categorical variables**"
      ],
      "metadata": {
        "id": "0cAtn1jprBk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pandas to encode us values as 1 and others as 0\n",
        "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x: 1 if x == 'us' else 0)\n",
        "\n",
        "# Print the number of unique type values\n",
        "print(len(ufo['type'].unique()))\n",
        "\n",
        "# Create a one-hot encoded set of the type values\n",
        "type_set = pd.get_dummies(ufo['type'])\n",
        "\n",
        "# Concatenate this set back to the ufo DataFrame\n",
        "ufo = pd.concat([ufo, type_set], axis=1)"
      ],
      "metadata": {
        "id": "UB1SLkQlrEBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features from date**"
      ],
      "metadata": {
        "id": "8BAPbnftrPL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the first 5 rows of the date column\n",
        "print(ufo['date'].head())\n",
        "\n",
        "# Extract the month from the date column\n",
        "ufo[\"month\"] = ufo[\"date\"].dt.month\n",
        "\n",
        "# Extract the year from the date column\n",
        "ufo[\"year\"] = ufo[\"date\"].dt.year\n",
        "\n",
        "# Take a look at the head of all three columns\n",
        "print(ufo[['date', 'month', 'year']].head())"
      ],
      "metadata": {
        "id": "_RoEoFRLrQ0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Vectorization**"
      ],
      "metadata": {
        "id": "83UyCKyNrfbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the head of the desc field\n",
        "print(ufo['desc'].head())\n",
        "\n",
        "# Instantiate the tfidf vectorizer object\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform desc using vec\n",
        "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
        "\n",
        "# Look at the number of columns and rows\n",
        "print(desc_tfidf.shape)"
      ],
      "metadata": {
        "id": "2akqsLDvrhUj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}